import unittest
from unittest.mock import patch, MagicMock

# Mock dbutils globally before importing any module that depends on it
import sys
from types import SimpleNamespace

mock_dbutils = SimpleNamespace(
    secrets=MagicMock(get=MagicMock(return_value="dummy_secret")),
    fs=MagicMock(ls=MagicMock(return_value=[MagicMock(name='device1'), MagicMock(name='device2')]))
)

sys.modules['dbutils'] = mock_dbutils

# Now import your script
from script_datarchival import (
    initialize_delta_lake_connection,
    get_device_list,
    filter_files_for_date,
    load_jsons_to_df,
    extract_dynamic_keys,
    write_to_delta,
)

class TestDataProcessingFunctions(unittest.TestCase):

    @patch('dbutils.secrets.get', return_value="dummy_service_credential")
    @patch('pyspark.sql.SparkSession.conf.set')
    def test_initialize_delta_lake_connection(self, mock_conf_set, mock_get):
        # Arrange
        storage_account = "test_storage"
        secret_scope = "test_scope"
        secret_key = "test_key"
        
        # Act
        initialize_delta_lake_connection(storage_account, secret_scope, secret_key)
        
        # Assert
        self.assertEqual(mock_conf_set.call_count, 5)  # Expecting five configuration methods to be called
    
    def test_get_device_list(self):
        # Arrange
        storage_account = "test_storage"
        container = "test_container"
        
        # Act
        devices = get_device_list(storage_account, container)
        
        # Assert
        self.assertEqual(len(devices), 2)

if __name__ == "__main__":
    unittest.main(argv=[''], exit=False)
