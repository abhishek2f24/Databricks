Here is a PySpark script to generate the two DataFrames with random data and random data types:

from pyspark.sql import SparkSession
from pyspark.sql.types import *
import random
from pyspark.sql.functions import col

# Initialize Spark Session
spark = SparkSession.builder.appName("RandomDataFrames").getOrCreate()

# Function to generate random schema
def generate_random_schema(prefix, num_cols):
    data_types = [IntegerType(), StringType(), FloatType(), BooleanType(), DoubleType(), TimestampType()]
    schema = StructType([
        StructField(f"{prefix}_column{i+1}", random.choice(data_types), True) 
        for i in range(num_cols)
    ])
    return schema

# Generate schema for both DataFrames
schema1 = generate_random_schema("table1", 60)
schema2 = generate_random_schema("table2", 10)

# Function to generate random row based on schema
def generate_random_row(schema):
    def random_value(data_type):
        if isinstance(data_type, IntegerType):
            return random.randint(1, 100)
        elif isinstance(data_type, StringType):
            return f"str_{random.randint(1, 1000)}"
        elif isinstance(data_type, FloatType):
            return round(random.uniform(1.0, 100.0), 2)
        elif isinstance(data_type, BooleanType):
            return random.choice([True, False])
        elif isinstance(data_type, DoubleType):
            return round(random.uniform(1.0, 100.0), 5)
        elif isinstance(data_type, TimestampType):
            from datetime import datetime, timedelta
            return datetime.now() - timedelta(days=random.randint(1, 365))
        else:
            return None

    return [random_value(field.dataType) for field in schema]

# Generate data for both DataFrames
data1 = [generate_random_row(schema1) for _ in range(5000)]
data2 = [generate_random_row(schema2) for _ in range(5000)]

# Create DataFrames
df1 = spark.createDataFrame(data1, schema=schema1)
df2 = spark.createDataFrame(data2, schema=schema2)

# Show schema and sample data
df1.printSchema()
df1.show(5)

df2.printSchema()
df2.show(5)

Explanation:

1. Random Schema Generation:

The generate_random_schema function picks random data types from a predefined list for each column.



2. Random Data Generation:

The generate_random_row function generates random values based on the column's data type.



3. Creating PySpark DataFrames:

We generate 5000 rows for both DataFrames and create them using spark.createDataFrame().




This will create two DataFrames with the required structure and random data. Let me know if you need any modifications!

